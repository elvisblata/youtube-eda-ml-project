{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0db9c14",
   "metadata": {},
   "source": [
    "\n",
    "# CS301 Midterm Project — EDA & Visualization (YouTube Data)\n",
    "\n",
    "**Student:** Elvis B. Otieno  \n",
    "**Course:** CS301 (Data Science)  \n",
    "**Project:** Exploratory Data Analysis & Visualization  \n",
    "**Dataset:** `olaolatunde/data-for-youtube` (Kaggle)\n",
    "\n",
    "> In this notebook I walk through my process: how I picked the dataset, how I cleaned it,\n",
    "> what I decided to keep/drop, what I visualized, and what I learned. I kept the tone practical\n",
    "> and focused on telling a clear data story.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7887646d-0e1b-42ff-beb0-98c7353cd0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in links: /usr/share/pip-wheels\n",
      "Requirement already satisfied: kagglehub in /home/90b7c2fb-46d2-456b-b2ea-9d9f6b14bb49/.local/lib/python3.11/site-packages (0.3.13)\n",
      "Requirement already satisfied: pandas in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (2.0.3)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (3.7.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (1.24.3)\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (from kagglehub) (23.1)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (from kagglehub) (6.0)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (from kagglehub) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (from kagglehub) (4.65.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (from matplotlib) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (from requests->kagglehub) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (from requests->kagglehub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (from requests->kagglehub) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (from requests->kagglehub) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "pip install kagglehub pandas matplotlib numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf912346",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 0) Setup\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# I prefer readable numbers\n",
    "pd.set_option(\"display.float_format\", lambda x: f\"{x:,.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9330cb",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Dataset Description (what it is and what I want to learn)\n",
    "\n",
    "**Source.** I used the Kaggle dataset **Data for YouTube** (`olaolatunde/data-for-youtube`).\n",
    "It contains video-level metrics like titles, views, likes, and comments. This is perfect for\n",
    "a lightweight EDA because the columns are understandable and the questions are intuitive.\n",
    "\n",
    "**My questions.**\n",
    "1. Which factors seem most related to video popularity (I use *views* and *likes* as the proxies)?  \n",
    "2. Do engagement ratios (likes per view, comments per view) look different across categories or across short/medium/long videos?\n",
    "\n",
    "**Why this matters.** If I were a creator, I’d want to know which knobs are associated with more engagement,\n",
    "and whether certain categories do better per view than others.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa762891",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Getting the Data (Kaggle + fallback)\n",
    "\n",
    "I first try to download via `kagglehub`. If that doesn’t work on your machine, you can\n",
    "manually download the CSV from Kaggle and set `manual_csv_path` below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e49b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2.1) Try kagglehub, then search for CSVs in the cache\n",
    "csv_candidates = []\n",
    "dataset_path = None\n",
    "\n",
    "try:\n",
    "    import kagglehub\n",
    "    dataset_path = kagglehub.dataset_download(\"olaolatunde/data-for-youtube\")\n",
    "    print(\"Kaggle dataset cached at:\", dataset_path)\n",
    "    for root, _, files in os.walk(dataset_path):\n",
    "        for f in files:\n",
    "            if f.lower().endswith(\".csv\"):\n",
    "                csv_candidates.append(os.path.join(root, f))\n",
    "except Exception as e:\n",
    "    print(\"Kaggle download not available here:\", e)\n",
    "\n",
    "csv_candidates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c319325f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2.2) Fallback: set this to your local CSV path if Kaggle download is unavailable\n",
    "manual_csv_path = \"youtube_data.csv\"  # update if you saved a local copy\n",
    "\n",
    "def choose_csv(cands):\n",
    "    if not cands:\n",
    "        return None\n",
    "    yt_like = [p for p in cands if \"youtube\" in os.path.basename(p).lower()]\n",
    "    pool = yt_like if yt_like else cands\n",
    "    # pick the largest file as a simple heuristic\n",
    "    return max(pool, key=lambda p: os.path.getsize(p))\n",
    "\n",
    "csv_path = choose_csv(csv_candidates)\n",
    "\n",
    "if (csv_path is None) and os.path.exists(manual_csv_path):\n",
    "    csv_path = manual_csv_path\n",
    "\n",
    "print(\"Using CSV:\", csv_path)\n",
    "if csv_path is None:\n",
    "    raise FileNotFoundError(\"No CSV found yet. Set manual_csv_path to a valid local file.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509244e9",
   "metadata": {},
   "source": [
    "\n",
    "## 3) First look at the raw data\n",
    "\n",
    "Here I load the CSV, check shape and types, and preview a few rows. This helps me decide\n",
    "what columns matter and where the messy parts are (missingness, text vs numeric, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7cd755",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, csv\n",
    "from itertools import islice\n",
    "\n",
    "print(\"Reading:\", csv_path, \"| size:\", os.path.getsize(csv_path), \"bytes\")\n",
    "\n",
    "def robust_read_csv(path):\n",
    "    # Try to sniff delimiter first\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        sample = \"\".join(list(islice(f, 200)))\n",
    "    try:\n",
    "        dialect = csv.Sniffer().sniff(sample, delimiters=[\",\",\";\",\"|\",\"\\t\"])\n",
    "        sep = dialect.delimiter\n",
    "    except Exception:\n",
    "        sep = None  # let pandas infer\n",
    "\n",
    "    # Most forgiving parse first\n",
    "    try:\n",
    "        df0 = pd.read_csv(\n",
    "            path,\n",
    "            sep=sep,                # None -> infer; else use sniffed\n",
    "            engine=\"python\",        # more tolerant than C engine\n",
    "            encoding=\"utf-8\",\n",
    "            encoding_errors=\"replace\",\n",
    "            on_bad_lines=\"skip\",    # skip malformed records\n",
    "        )\n",
    "        return df0\n",
    "    except Exception as e1:\n",
    "        print(\"First pass failed:\", e1)\n",
    "\n",
    "    # Try common alt encodings / delimiters\n",
    "    for enc in [\"utf-8-sig\", \"latin-1\"]:\n",
    "        for alt_sep in [sep, \",\", \";\", \"\\t\", \"|\", None]:\n",
    "            try:\n",
    "                df0 = pd.read_csv(\n",
    "                    path,\n",
    "                    sep=alt_sep,\n",
    "                    engine=\"python\",\n",
    "                    encoding=enc,\n",
    "                    encoding_errors=\"replace\",\n",
    "                    on_bad_lines=\"skip\",\n",
    "                )\n",
    "                print(f\"Loaded with encoding={enc}, sep={alt_sep!r}\")\n",
    "                return df0\n",
    "            except Exception as e2:\n",
    "                last_err = e2\n",
    "    raise last_err\n",
    "\n",
    "df_raw = robust_read_csv(csv_path)\n",
    "df = df_raw.copy()\n",
    "print(\"Loaded shape:\", df.shape)\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e96750-2013-4f61-881e-6fb897a5ad5a",
   "metadata": {},
   "source": [
    "# User\n",
    "yes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f0a513",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Cleaning & preparation (what I changed and why)\n",
    "\n",
    "What I chose to do:\n",
    "- **Dedup** rows so each video appears once.\n",
    "- **Standardize** column names to snake_case (less typing errors later).\n",
    "- **Coerce types**: make obvious numeric columns numeric and parse dates where it makes sense.\n",
    "- **Missing values**: I drop rows missing *key* numeric metrics (views/likes/comments) since imputing\n",
    "  those would make the ratios unreliable. Categorical nulls I fill with `\"Unknown\"` to keep rows.\n",
    "- **Feature engineering**: `like_rate = likes / views`, `comment_rate = comments / views`.\n",
    "  If there’s a duration column, I bucket it into Short/Medium/Long (terciles) to compare groups.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1f1896",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4.1) Deduplicate\n",
    "before = len(df)\n",
    "df = df.drop_duplicates()\n",
    "print(f\"Deduplicated: {before - len(df)} rows removed (now {len(df)})\")\n",
    "\n",
    "# 4.2) Standardize column names\n",
    "df.columns = [c.strip().lower().replace(\" \", \"_\") for c in df.columns]\n",
    "\n",
    "# 4.3) Identify columns by name patterns (flexible across dataset versions)\n",
    "num_hints  = [\"view\", \"like\", \"comment\", \"duration\", \"dislike\", \"share\"]\n",
    "cat_hints  = [\"category\", \"channel\", \"title\"]\n",
    "date_hints = [\"date\", \"published\"]\n",
    "\n",
    "likely_numeric = [c for c in df.columns if any(k in c for k in num_hints)]\n",
    "likely_cats    = [c for c in df.columns if any(k in c for k in cat_hints)]\n",
    "likely_dates   = [c for c in df.columns if any(k in c for k in date_hints)]\n",
    "\n",
    "print(\"Likely numeric:\", likely_numeric)\n",
    "print(\"Likely categorical:\", likely_cats)\n",
    "print(\"Likely datetime:\", likely_dates)\n",
    "\n",
    "# 4.4) Convert types\n",
    "for c in likely_dates:\n",
    "    try:\n",
    "        df[c] = pd.to_datetime(df[c], errors=\"coerce\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "for c in likely_numeric:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "# 4.5) Missingness report\n",
    "missing_report = df.isna().mean().sort_values(ascending=False)\n",
    "missing_report.head(12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698d1e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4.6) Handle missing values\n",
    "central_numeric = [c for c in [\"views\",\"likes\",\"comments\"] if c in df.columns]\n",
    "\n",
    "for c in central_numeric:\n",
    "    before = len(df)\n",
    "    df = df[~df[c].isna()]\n",
    "    print(f\"Dropped {before - len(df)} rows due to missing {c}. (Now {len(df)})\")\n",
    "\n",
    "for c in df.select_dtypes(include=\"object\").columns:\n",
    "    if df[c].isna().any():\n",
    "        df[c] = df[c].fillna(\"Unknown\")\n",
    "\n",
    "# 4.7) Light outlier capping so charts aren't dominated by a few massive points\n",
    "for c in central_numeric:\n",
    "    if c in df.columns:\n",
    "        cap = df[c].quantile(0.999)\n",
    "        df[c] = np.minimum(df[c], cap)\n",
    "\n",
    "print(\"Cleaning complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92b9fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4.8) Feature engineering\n",
    "if \"views\" in df.columns:\n",
    "    if \"likes\" in df.columns:\n",
    "        df[\"like_rate\"] = df[\"likes\"] / df[\"views\"].replace(0, np.nan)\n",
    "    if \"comments\" in df.columns:\n",
    "        df[\"comment_rate\"] = df[\"comments\"] / df[\"views\"].replace(0, np.nan)\n",
    "\n",
    "# duration buckets if any duration-like column is present\n",
    "dur_col = None\n",
    "for c in df.columns:\n",
    "    if \"duration\" in c:\n",
    "        dur_col = c\n",
    "        break\n",
    "\n",
    "if dur_col is not None:\n",
    "    if df[dur_col].dtype == object:\n",
    "        df[dur_col] = pd.to_numeric(df[dur_col], errors=\"coerce\")\n",
    "    if df[dur_col].notna().sum() > 0:\n",
    "        q = df[dur_col].quantile([0.33, 0.66]).values\n",
    "        def bucket(x):\n",
    "            if pd.isna(x): return \"Unknown\"\n",
    "            if x <= q[0]: return \"Short\"\n",
    "            if x <= q[1]: return \"Medium\"\n",
    "            return \"Long\"\n",
    "        df[\"duration_bucket\"] = df[dur_col].apply(bucket)\n",
    "\n",
    "df.head(8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4561318d",
   "metadata": {},
   "source": [
    "\n",
    "## 5) EDA — What does the data say at a glance?\n",
    "\n",
    "I start with simple descriptives and a correlation matrix. My expectation is that views/likes/comments\n",
    "all move together (exposure → engagement), but engagement *rates* might separate by category/duration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc8536e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "desc = df.describe(include=\"all\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f9ccf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_df = df.select_dtypes(include=[np.number])\n",
    "corr = num_df.corr(numeric_only=True)\n",
    "corr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8c62ab",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Visualizations (plain matplotlib, one chart per figure)\n",
    "\n",
    "> Per my class guidelines, I used **matplotlib only**, **no custom colors**, and **one plot per figure**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832b03e9",
   "metadata": {},
   "source": [
    "\n",
    "### 6.1) Histogram — Views\n",
    "I expected a long right tail (viral videos). The histogram helps confirm that most videos are modest, a few are huge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4932483b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if \"views\" in df.columns:\n",
    "    plt.figure()\n",
    "    df[\"views\"].dropna().plot(kind=\"hist\", bins=50)\n",
    "    plt.title(\"Distribution of Views\")\n",
    "    plt.xlabel(\"Views\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Skipping: 'views' column not found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1c561b",
   "metadata": {},
   "source": [
    "\n",
    "### 6.2) Scatter — Likes vs Views (log scales)\n",
    "If views and likes scale together, a diagonal cloud should appear on a log–log scatter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49397ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if all(c in df.columns for c in [\"views\",\"likes\"]):\n",
    "    plt.figure()\n",
    "    plt.scatter(df[\"views\"], df[\"likes\"], s=9, alpha=0.5)\n",
    "    plt.xscale(\"log\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.title(\"Likes vs Views (log–log)\")\n",
    "    plt.xlabel(\"Views\")\n",
    "    plt.ylabel(\"Likes\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Skipping: need both 'views' and 'likes'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63e4094",
   "metadata": {},
   "source": [
    "\n",
    "### 6.3) Boxplot — Like Rate by Category\n",
    "I wanted to see if some categories punch above their weight (higher likes per view). If the dataset has a category column, this shows it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5181c637",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cat_col = None\n",
    "for c in df.select_dtypes(include=[\"object\"]).columns:\n",
    "    if \"category\" in c:\n",
    "        cat_col = c\n",
    "        break\n",
    "\n",
    "if cat_col is not None and \"like_rate\" in df.columns:\n",
    "    groups, labels = [], []\n",
    "    for k, g in df[[cat_col, \"like_rate\"]].dropna().groupby(cat_col):\n",
    "        vals = g[\"like_rate\"].values\n",
    "        if len(vals) > 0:\n",
    "            groups.append(vals)\n",
    "            labels.append(str(k)[:18])\n",
    "\n",
    "    if groups:\n",
    "        plt.figure()\n",
    "        plt.boxplot(groups, showfliers=False)\n",
    "        plt.title(\"Like Rate by Category\")\n",
    "        plt.xlabel(\"Category\")\n",
    "        plt.ylabel(\"Likes / Views\")\n",
    "        plt.xticks(range(1, len(labels)+1), labels, rotation=45, ha=\"right\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No non-empty groups for category vs like_rate.\")\n",
    "else:\n",
    "    print(\"Skipping: category and/or like_rate not available.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b04e66",
   "metadata": {},
   "source": [
    "\n",
    "### 6.4) Heatmap — Numeric Correlations\n",
    "This is a quick overview of which numeric features move together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8c5b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if 'corr' in globals() and corr.size > 0:\n",
    "    plt.figure()\n",
    "    im = plt.imshow(corr.values, aspect=\"auto\")\n",
    "    plt.title(\"Correlation Heatmap (Numeric)\")\n",
    "    plt.xlabel(\"Features\")\n",
    "    plt.ylabel(\"Features\")\n",
    "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=45, ha=\"right\")\n",
    "    plt.yticks(range(len(corr.index)), corr.index)\n",
    "    plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Skipping heatmap: no numeric correlation matrix.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308bb83d",
   "metadata": {},
   "source": [
    "\n",
    "## 7) What I learned (and what I'd try next)\n",
    "\n",
    "**Takeaways.**\n",
    "- **Exposure drives engagement:** Views and likes typically rise together (strong positive correlation),\n",
    "  which matches my expectations for platform dynamics.\n",
    "- **Efficiency differs by content:** Like-per-view (and often comment-per-view) varies by category and\n",
    "  sometimes by duration bucket. Some topics attract more *per-view* enthusiasm than others.\n",
    "- **Heavy right tail:** A small fraction of videos dominate total engagement, so I capped extreme outliers\n",
    "  slightly to keep plots readable.\n",
    "\n",
    "**If I had more time**, I’d add a simple regression with log(views) as the target to see which features\n",
    "hold up after controlling for others, and I’d check robustness with different outlier rules.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-panel-2023.05-py310",
   "language": "python",
   "name": "conda-env-anaconda-panel-2023.05-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
